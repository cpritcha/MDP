{
 "metadata": {
  "language": "Julia",
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Markov Decision Process (MDP)\n",
      "\n",
      "## Infinite Horizon Dynamic Programming\n",
      "\n",
      "### Overview\n",
      "\n",
      "In dynamic programming an agent finds a policy (an action given their current state) that maximizes their present discounted award given\n",
      "\n",
      "\\begin{align*}\n",
      "    \\beta &\\quad\\text{discount rate}\\\\\n",
      "    R_{ik} &\\quad\\text{reward from transitioning to $i$ using control $k$} \\\\\n",
      "    P_{ijk} &\\quad\\text{probability of transitioning from state $i$ to $j$ using control $k$} \\\\\n",
      "    V_{i} &\\quad\\text{present discounted value of being in state $i$}\n",
      "\\end{align*}\n",
      "\n",
      "### Value Iteration\n",
      "\n",
      "Value iteration is a simple way to estimate discrete infinite horizon dynamic programs. In value iteration we set our present discounted value of being in a particular state to arbitrary values and iterate on the Bellman equation until convergence\n",
      "\n",
      "\\begin{align*}\n",
      "    &\\text{do} \\\\\n",
      "        &\\quad i += 1\\\\\n",
      "        &\\quad V_{i+1} = \\max (R + \\beta PV_i) \\\\\n",
      "    &\\text{until}\\quad || V_{i+1} - V_i || < \\epsilon\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Policy Iteration\n",
      "\n",
      "In policy iteration we start with an initial policy and then find the present discounted value $V$ of the policy. Then the policy that maximizes the payoff for next period is taken. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Examples\n",
      "\n",
      "[fml]: http://www.cs.nyu.edu/~mohri/mlbook/ \"Foundations of Machine Learning\"\n",
      "Following an example given in [Foundations of Machine Learning][fml] suppose we have a infinite horizon dynamic program identical to the one given in the graph shown below:\n",
      "\n",
      "<img src=\"files/mdp.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice the edges in the graph correspond have probabilities and rewards. For instance, if you choose to play $a$ in state 1 there is a 75% chance that you will end up in state 1 (with a reward of 2) and 25% percent chance that you will end up in state 2 (also with a reward of 2).\n",
      "\n",
      "In Julia this problem can be represented using arrays "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using MDP\n",
      "\n",
      "I = [1,1,2,3,4]\n",
      "J = [1,2,2,2,1]\n",
      "R = sparse(I, J, Float64[2,2,2,2,3])\n",
      "P = sparse(I,J,[.75,.25,1.00,1.00,1.00])\n",
      "indvec = [2,4]\n",
      "# indvec describes what choices correspond to each state\n",
      "# the \"2\" says that the first to second elements of R correspond to playing \"a\" or \"b\" in state 1\n",
      "# the \"4\" says that the third to fourth elements of R correspond to playing \"c\" or \"d\" in state 2\n",
      "V0 = Float64[-1; 1]\n",
      "\u03b2=0.5\n",
      "\n",
      "valueiteration(mdp; Vstart=V0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "([4.66602,5.33276],[2,4])"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output of value iteration shows that the optimal policy is to play $b$ in state 1 and $d$ in state 2. The policy makes intuitive sense given the above graph because the agent wants to maximize the chance of getting the high reward choice $d$. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "policyiteration(mdp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "([4.66667,5.33333],[2,4])"
       ]
      }
     ],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}